2022-09-15 02:20:18,027 - tnorm_latent_variables.py - INFO: 
 ['px, py1, py0:', 0.7929833398533687, 0.265492447178147, 0.3161628060859345]
2022-09-15 02:20:18,027 - tnorm_latent_variables.py - INFO: 
 ['data:',     ~X | ~Y  X | Y  X & Y  X & ~Y
0         1      1      0       0
1         1      1      0       0
2         1      1      0       0
3         1      1      0       0
4         0      1      1       0
..      ...    ...    ...     ...
95        1      0      0       0
96        1      0      0       0
97        1      1      0       0
98        0      1      1       0
99        1      0      0       0

[100 rows x 4 columns]]
2022-09-15 02:20:18,037 - tnorm_latent_variables.py - INFO: 
 ['Training data: ',     ~X | ~Y  X | Y  X & Y  X & ~Y
2         1      1      0       0
73        1      1      0       0
97        1      1      0       0
62        1      0      0       0
19        1      0      0       0
..      ...    ...    ...     ...
75        1      1      0       0
9         1      1      0       0
72        1      0      0       0
12        1      1      0       0
37        1      1      0       0

[80 rows x 4 columns]]
2022-09-15 02:20:18,043 - tnorm_latent_variables.py - INFO: 
 ['Testing data: ',     ~X | ~Y  X | Y  X & Y  X & ~Y
80        0      1      1       0
84        1      1      0       0
33        1      1      0       0
81        1      1      0       0
93        1      1      0       0
17        1      1      0       1
36        1      1      0       0
82        1      0      0       0
69        0      1      1       0
65        0      1      1       0
92        1      1      0       0
39        1      0      0       0
56        0      1      1       0
52        1      1      0       0
51        1      1      0       0
32        1      1      0       0
31        1      1      0       0
44        1      1      0       0
78        1      1      0       0
10        1      1      0       0]
2022-09-15 02:20:18,049 - tnorm_latent_variables.py - INFO: 
 ['Training shape:', (80, 4), 'Testing shape: ', (20, 4)]
2022-09-15 02:20:18,230 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:20:18,230 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 0]
2022-09-15 02:20:18,230 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 39.92924531461993]
2022-09-15 02:20:18,231 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1428, requires_grad=True)]
2022-09-15 02:20:18,232 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.4462, requires_grad=True)]
2022-09-15 02:20:18,234 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.4027, requires_grad=True)]
2022-09-15 02:20:18,253 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:20:18,253 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 0]
2022-09-15 02:20:18,254 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 12.693965467624366]
2022-09-15 02:20:18,255 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1428, requires_grad=True)]
2022-09-15 02:20:18,257 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.4462, requires_grad=True)]
2022-09-15 02:20:18,258 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.4027, requires_grad=True)]
2022-09-15 02:20:33,321 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:20:33,321 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 80]
2022-09-15 02:20:33,321 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:20:33,323 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:20:33,324 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:20:33,325 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:20:33,357 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:20:33,358 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 80]
2022-09-15 02:20:33,358 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:20:33,360 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:20:33,362 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:20:33,364 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:20:49,154 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:20:49,154 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 160]
2022-09-15 02:20:49,154 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:20:49,155 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:20:49,157 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:20:49,159 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:20:49,178 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:20:49,178 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 160]
2022-09-15 02:20:49,178 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:20:49,180 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:20:49,181 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:20:49,182 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:05,664 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:05,664 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 240]
2022-09-15 02:21:05,664 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:21:05,665 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:05,667 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:05,669 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:05,700 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:05,700 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 240]
2022-09-15 02:21:05,700 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:21:05,702 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:05,704 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:05,706 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:23,046 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:23,047 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 320]
2022-09-15 02:21:23,047 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:21:23,048 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:23,049 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:23,050 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:23,070 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:23,070 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 320]
2022-09-15 02:21:23,070 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:21:23,072 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:23,073 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:23,074 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:39,167 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:39,168 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 400]
2022-09-15 02:21:39,168 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:21:39,169 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:39,170 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:39,172 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:39,195 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:39,196 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 400]
2022-09-15 02:21:39,196 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:21:39,197 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:39,199 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:39,200 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:58,837 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:58,837 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 480]
2022-09-15 02:21:58,837 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:21:58,838 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:58,840 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:58,842 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:21:58,861 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:21:58,861 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 480]
2022-09-15 02:21:58,862 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:21:58,863 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:21:58,864 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:21:58,866 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:14,209 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:22:14,209 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 560]
2022-09-15 02:22:14,209 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:22:14,211 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:22:14,212 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:22:14,213 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:14,234 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:22:14,235 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 560]
2022-09-15 02:22:14,235 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:22:14,236 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:22:14,238 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:22:14,239 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:29,517 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:22:29,517 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 640]
2022-09-15 02:22:29,517 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:22:29,518 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:22:29,520 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:22:29,521 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:29,540 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:22:29,540 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 640]
2022-09-15 02:22:29,541 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:22:29,542 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:22:29,543 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:22:29,544 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:44,586 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:22:44,587 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 720]
2022-09-15 02:22:44,587 - tnorm_latent_variables.py - INFO: 
 ['Training batch loss = ', 38.7979728576247]
2022-09-15 02:22:44,588 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:22:44,590 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:22:44,591 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:44,615 - tnorm_latent_variables.py - INFO: 
 -------------------------------------------------------------------------
2022-09-15 02:22:44,616 - tnorm_latent_variables.py - INFO: 
 ['batch = ', 720]
2022-09-15 02:22:44,616 - tnorm_latent_variables.py - INFO: 
 ['Validation batch loss = ', 11.393089867662638]
2022-09-15 02:22:44,617 - tnorm_latent_variables.py - INFO: 
 ['px', ':', Parameter containing:
tensor(0.1391, requires_grad=True)]
2022-09-15 02:22:44,618 - tnorm_latent_variables.py - INFO: 
 ['py', ':', Parameter containing:
tensor(0.5536, requires_grad=True)]
2022-09-15 02:22:44,620 - tnorm_latent_variables.py - INFO: 
 ['dependency', ':', Parameter containing:
tensor(0.3499, requires_grad=True)]
2022-09-15 02:22:59,632 - tnorm_latent_variables.py - INFO: 
 ['Training loss:', tensor([39.9292, 38.8025, 38.7887, 38.7949, 38.7969, 38.7976, 38.7979, 38.7979,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980,
        38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980, 38.7980])]
2022-09-15 02:22:59,638 - tnorm_latent_variables.py - INFO: 
 ['Testing Loss:', tensor([0.0084, 0.0074, 0.0074, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073,
        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073])]
2022-09-15 02:22:59,643 - tnorm_latent_variables.py - INFO: 
 ['norms:', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])]
2022-09-15 02:22:59,645 - tnorm_latent_variables.py - INFO: 
 Training complete.
